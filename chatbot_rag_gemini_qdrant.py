import os
import streamlit as st
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Tuple, Dict, Any
import re
import json
import faiss
from dotenv import load_dotenv

# ThÆ° viá»‡n Gemini
try:
    import google.generativeai as genai
    HAS_GEMINI = True
except Exception:
    HAS_GEMINI = False

# ---------- Load ENV ----------
load_dotenv()  # náº¡p biáº¿n mÃ´i trÆ°á»ng tá»« file .env náº¿u cÃ³
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ---------- Configuration & File Names ----------
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'
JSON_FILE = 'so_tay_sinh_vien_k65.json'

# ---------- Core Utilities ----------

@st.cache_resource
def get_embedding_model(name=EMBEDDING_MODEL_NAME):
    """Táº£i vÃ  cache mÃ´ hÃ¬nh nhÃºng."""
    return SentenceTransformer(name)

def l2_normalize_rows(x: np.ndarray) -> np.ndarray:
    """Chuáº©n hÃ³a vector vá» Ä‘á»™ dÃ i Ä‘Æ¡n vá»‹."""
    norms = np.linalg.norm(x, axis=1, keepdims=True)
    norms[norms == 0] = 1e-10
    return x / norms

def load_all_data_from_json(json_file_path: str) -> Tuple[List[Dict[str, Any]], Dict[int, str]]:
    """Táº£i dá»¯ liá»‡u tá»« JSON, táº¡o metadata cho FAISS vÃ  map ná»™i dung trang."""
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
    except FileNotFoundError:
        st.error(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file JSON: {json_file_path}. Vui lÃ²ng Ä‘áº£m báº£o file tá»“n táº¡i.")
        return [], {}
    except json.JSONDecodeError:
        st.error(f"Lá»—i: File {json_file_path} bá»‹ lá»—i Ä‘á»‹nh dáº¡ng JSON.")
        return [], {}
    
    metadata_list = []
    page_content_map = {}
    
    for item in raw_data:
        page_num = item.get("page")
        full_content = item.get("content", "").strip()
        
        if not page_num or not full_content:
            continue
            
        # DÃ¹ng dÃ²ng Ä‘áº§u tiÃªn (sau khi lÃ m sáº¡ch noise) lÃ m tiÃªu Ä‘á»/mÃ´ táº£ cho nhÃºng
        lines = [line.strip() for line in full_content.split('\n') if line.strip()]
        
        # Lá»c bá» cÃ¡c dÃ²ng noise á»Ÿ Ä‘áº§u (vÃ­ dá»¥: '2 *', '4 *')
        title = ""
        for line in lines:
            if not re.match(r"^\s*\d+\s*\*|^\s*$", line):
                title = line
                break
        
        if not title:
            title = f"Ná»™i dung trang {page_num}"
        
        if len(title) > 200:
            title = title[:150] + "..."

        metadata_list.append({
            "id": f"page_{page_num}",
            "content": title,           # TiÃªu Ä‘á»/mÃ´ táº£ Ä‘á»ƒ nhÃºng
            "page": page_num,
            "full_content": full_content # Ná»™i dung Ä‘áº§y Ä‘á»§
        })
        
        page_content_map[page_num] = full_content
        
    return metadata_list, page_content_map

def ensure_session_state():
    if 'faiss_index' not in st.session_state:
        st.session_state['faiss_index'] = None
    if 'messages' not in st.session_state:
        st.session_state['messages'] = []
        
    # Táº£i dá»¯ liá»‡u JSON náº¿u chÆ°a cÃ³
    if 'stored_metadata' not in st.session_state or not st.session_state.get('stored_metadata', []):
        metadata_list, page_content_map = load_all_data_from_json(JSON_FILE)
        st.session_state['stored_metadata'] = metadata_list
        st.session_state['page_content_map'] = page_content_map
    
    if 'page_content_map' not in st.session_state:
        st.session_state['page_content_map'] = {}

# ---------- FAISS Utilities ----------

def upsert_metadata_faiss(metadata_list: List[Dict[str, Any]], model: SentenceTransformer):
    ensure_session_state()
    
    if not metadata_list:
        return
        
    titles = [item['content'] for item in metadata_list]
    
    embeddings = model.encode(titles, show_progress_bar=True)
    embeddings = np.asarray(embeddings).astype('float32')
    embeddings = l2_normalize_rows(embeddings)
    
    dim = embeddings.shape[1]
    
    idx = faiss.IndexFlatIP(dim)
    st.session_state['faiss_index'] = idx
    idx.add(embeddings)

def retrieve_faiss_metadata(query: str, model: SentenceTransformer, top_k: int = 5) -> List[Tuple[Dict[str, Any], float]]:
    ensure_session_state()
    idx = st.session_state.get('faiss_index', None)
    metadata = st.session_state.get('stored_metadata', [])
    
    if idx is None or idx.ntotal == 0:
        return []
        
    q_emb = model.encode([query])[0].astype('float32').reshape(1, -1)
    q_emb = l2_normalize_rows(q_emb)
    
    k = min(top_k, idx.ntotal)
    if k == 0:
        return []
    
    scores, ids = idx.search(q_emb, k)
    results = []
    for idx_id, score in zip(ids[0], scores[0]):
        if 0 <= idx_id < len(metadata):
            results.append((metadata[idx_id], float(score)))
    return results

def rerank_by_cosine_metadata(query: str, candidates: List[Dict[str, Any]], model: SentenceTransformer) -> List[Tuple[Dict[str, Any], float]]:
    if not candidates:
        return []
        
    contents = [c['content'] for c in candidates]
    q_emb = model.encode([query])[0].reshape(1, -1)
    c_embs = model.encode(contents)
    
    sims = cosine_similarity(q_emb, c_embs)[0]
    
    paired = list(zip(candidates, sims.tolist()))
    paired.sort(key=lambda x: x[1], reverse=True)
    return paired

# ---------- Prompt & Gemini Call Utilities ----------

def build_prompt(question, documents: List[Tuple[Dict[str, Any], float]]):
    context_parts = []
    
    for i, (doc_meta, score) in enumerate(documents):
        page_num = doc_meta['page']
        content = doc_meta.get('full_content', f"[Lá»–I: KhÃ´ng tÃ¬m tháº¥y ná»™i dung Ä‘áº§y Ä‘á»§ cho trang {page_num}]")
        context_parts.append(f"TÃ i liá»‡u liÃªn quan (Trang {page_num}):\n{content}")
        
    context = "\n\n".join(context_parts)
    
    prompt = f"""
Báº¡n lÃ  má»™t trá»£ lÃ½ áº£o. TÃ´i sáº½ Ä‘Æ°a cho báº¡n má»™t cÃ¢u há»i vÃ  má»™t sá»‘ Ä‘oáº¡n tÃ i liá»‡u (Ná»™i dung Trang) cÃ³ cáº¥u trÃºc.
HÃ£y tráº£ lá»i cÃ¢u há»i Má»˜T CÃCH NGáº®N Gá»ŒN, TRá»°C TIáº¾P vÃ  SÃšC TÃCH, chá»‰ dá»±a trÃªn ná»™i dung CÃ“ trong cÃ¡c Ä‘oáº¡n tÃ i liá»‡u sau.
Tuyá»‡t Ä‘á»‘i KHÃ”NG bao gá»“m cÃ¡c tham chiáº¿u Ä‘áº¿n sá»‘ trang, tiÃªu Ä‘á» hay báº¥t ká»³ ghi chÃº nÃ o vá» tÃ i liá»‡u nguá»“n (vÃ­ dá»¥: 'Theo tÃ i liá»‡u trang 5...', 'DOC 1 nÃ³i ráº±ng...').
Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§ Ä‘á»ƒ tráº£ lá»i hoÃ n chá»‰nh, hÃ£y tráº£ lá»i pháº§n cÃ³ thá»ƒ vÃ  nÃ³i rÃµ "chÆ°a Ä‘á»§ dá»¯ liá»‡u".
Náº¿u KHÃ”NG CÃ“ thÃ´ng tin liÃªn quan, hÃ£y tráº£ lá»i "KhÃ´ng tÃ¬m tháº¥y trong tÃ i liá»‡u".
Tuyá»‡t Ä‘á»‘i KHÃ”NG suy diá»…n hoáº·c thÃªm thÃ´ng tin ngoÃ i tÃ i liá»‡u.

CÃ¢u há»i: {question}

CÃ¡c Ä‘oáº¡n tÃ i liá»‡u:
{context}

CÃ¢u tráº£ lá»i:
"""
    return prompt

def call_gemini(prompt: str, model_name: str = "gemini-1.5-flash") -> str:
    """Gá»i Gemini API vá»›i API Key tá»« ENV."""
    api_key = GEMINI_API_KEY
         
    if not api_key:
        return "âŒ Lá»—i cáº¥u hÃ¬nh: KhÃ´ng tÃ¬m tháº¥y Gemini API Key. Vui lÃ²ng kiá»ƒm tra láº¡i file `.env`."
    if not HAS_GEMINI:
        return "âŒ ThÆ° viá»‡n google-generativeai chÆ°a cÃ i. Cháº¡y: pip install google-generativeai"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)
        
        response = model.generate_content(prompt)
        
        if hasattr(response, 'text'):
            return response.text
        
        return "Lá»—i pháº£n há»“i: KhÃ´ng láº¥y Ä‘Æ°á»£c ná»™i dung vÄƒn báº£n."

    except Exception as e:
        return f"[Gemini Error] Lá»—i gá»i API: {e}"

# ---------- Streamlit UI & Main Logic ----------
ensure_session_state()

st.set_page_config(page_title='Chatbot RAG - Dá»¯ liá»‡u JSON & Tá»± Ä‘á»™ng Táº£i', layout='wide')
st.title('Chatbot RAG - Sá»• tay Sinh viÃªn (Tá»± Ä‘á»™ng táº£i dá»¯ liá»‡u JSON)')

metadata_list = st.session_state.get('stored_metadata', [])

# --- Cáº¥u hÃ¬nh Sidebar ---
with st.sidebar:
    st.header('Cáº¥u hÃ¬nh')
    if GEMINI_API_KEY:
        st.success('âœ… Gemini API Key Ä‘Ã£ Ä‘Æ°á»£c náº¡p tá»« biáº¿n mÃ´i trÆ°á»ng.')
    else:
        st.error('âŒ Thiáº¿u Gemini API Key. Vui lÃ²ng táº¡o file `.env` hoáº·c set biáº¿n mÃ´i trÆ°á»ng.')
        
    top_k = st.number_input('Top-K Metadata (cho Rerank)', value=5, min_value=1, max_value=20, key='top_k_widget')
    model_choice = st.selectbox('Model Gemini (náº¿u dÃ¹ng)', options=['gemini-1.5-flash', 'gemini-1.5-pro'], index=0)

    if st.button('XÃ³a FAISS & Reset Chat'):
        st.session_state['faiss_index'] = None
        st.session_state['messages'] = []
        st.rerun()

# --- 1. Táº£i vÃ  NhÃºng Dá»¯ liá»‡u Tá»± Ä‘á»™ng ---
st.subheader('1. Táº£i vÃ  NhÃºng Dá»¯ liá»‡u')

if not metadata_list:
    st.error(f"âš ï¸ **KhÃ´ng tÃ¬m tháº¥y hoáº·c metadata bá»‹ lá»—i trong file: `{JSON_FILE}`.** Vui lÃ²ng kiá»ƒm tra láº¡i file vÃ  cáº¥u trÃºc JSON.")
else:
    st.info(f'âœ… **ÄÃ£ táº£i thÃ nh cÃ´ng {len(metadata_list)} má»¥c metadata** tá»« file `{JSON_FILE}`.')
    st.caption('TiÃªu Ä‘á» (Content) cá»§a má»—i má»¥c Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng lÃ m sáº¡ch tá»« dÃ²ng Ä‘áº§u tiÃªn cá»§a trang.')

    if st.button('NhÃºng Metadata vÃ o FAISS', key='save_metadata_button'):
        with st.spinner('Äang nhÃºng TIÃŠU Äá»€ trang vÃ o FAISS...'):
            try:
                model = get_embedding_model()
                upsert_metadata_faiss(metadata_list, model) 
                st.success('âœ… ÄÃ£ lÆ°u Metadata (TiÃªu Ä‘á») vÃ o FAISS.')
                st.caption(f'Tá»•ng má»¥c Metadata hiá»‡n cÃ³: **{len(st.session_state["stored_metadata"])}**')
            except Exception as e:
                st.error(f"Lá»—i khi nhÃºng/lÆ°u FAISS: {e}")

# --- 2. Chatbot tÆ°Æ¡ng tÃ¡c ---
st.subheader('2. Chatbot tÆ°Æ¡ng tÃ¡c')
trace_expander = st.expander("ðŸ›  Xem chi tiáº¿t Trace/Debug (Metadata, Prompt)", expanded=False)

# --- HIá»‚N THá»Š Lá»ŠCH Sá»¬ CHAT ---
is_faiss_ready = st.session_state['faiss_index'] is not None

if not st.session_state.messages and is_faiss_ready:
    st.session_state.messages.append({"role": "assistant", "content": "ChÃ o báº¡n! HÃ£y há»i tÃ´i vá» Sá»• tay sinh viÃªn Ä‘Ã£ Ä‘Æ°á»£c nhÃºng dá»¯ liá»‡u."})
elif not st.session_state.messages:
    st.session_state.messages.append({"role": "assistant", "content": "ChÃ o báº¡n! Vui lÃ²ng nháº¥n **NhÃºng Metadata vÃ o FAISS** á»Ÿ pháº§n 1 trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i."})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Xá»¬ LÃ Äáº¦U VÃ€O Má»šI ---
prompt = st.chat_input("Há»i vá» tÃ i liá»‡u cá»§a báº¡n...", key="main_chat_input")

if prompt:
    if not is_faiss_ready:
        st.error("Vui lÃ²ng nháº¥n **NhÃºng Metadata vÃ o FAISS** á»Ÿ pháº§n 1 trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner('ðŸ¤– Äang tÃ¬m kiáº¿m Metadata vÃ  tráº£ lá»i...'):
                model = get_embedding_model()
                
                results = retrieve_faiss_metadata(prompt, model, top_k=top_k * 2) 
                
                if not results:
                    answer = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong FAISS."
                else:
                    candidates_meta = [r[0] for r in results]
                    reranked = rerank_by_cosine_metadata(prompt, candidates_meta, model)
                    
                    retrieved_for_prompt = reranked[:top_k]
                    
                    if not retrieved_for_prompt:
                        answer = "KhÃ´ng tÃ¬m tháº¥y Metadata tÆ°Æ¡ng tá»± sau khi rerank."
                    else:
                        final_prompt = build_prompt(prompt, retrieved_for_prompt)
                        
                        if GEMINI_API_KEY and HAS_GEMINI:
                            answer = call_gemini(final_prompt, model_name=model_choice)
                        else:
                            # Fallback mode
                            joined_docs = "\n\n".join([
                                f"**Má»¥c {i+1} - {t[0]['content']} (Trang {t[0]['page']}, Sim={t[1]:.4f})**\n{t[0]['full_content']}" 
                                for i, t in enumerate(retrieved_for_prompt)
                            ])
                            answer = f"**Cháº¿ Ä‘á»™ DEMO (Lá»—i API Key)**\n\n**TÃ³m táº¯t dá»¯ liá»‡u liÃªn quan:**\n{joined_docs[:2000]}...\n\n_Vui lÃ²ng kiá»ƒm tra láº¡i API Key Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i tá»± nhiÃªn hÆ¡n._"

                    with trace_expander:
                        st.markdown('--- ðŸ“‘ Top Metadata sau rerank ---')
                        for i, (meta, sim) in enumerate(retrieved_for_prompt):
                            st.markdown(f"**Má»¥c {i+1}** â€” similarity (rerank)={sim:.4f}")
                            st.code(f"TiÃªu Ä‘á»: {meta['content']} (Trang {meta['page']})\n\nNá»™i dung (Äáº§y Ä‘á»§):\n{meta['full_content'][:800]}...", language='markdown')
                        
                        st.write('--- Prompt (tÃ³m táº¯t, chá»‰ hiá»ƒn thá»‹ pháº§n Ä‘áº§u) ---')
                        st.code(final_prompt[:3000] + ('...' if len(final_prompt) > 3000 else ''), language='markdown')

                        st.write('ðŸ›  Trace / Debug info', {
                            'stored_metadata_count': len(st.session_state.get('stored_metadata', [])),
                            'faiss_index_ntotal': 0 if st.session_state['faiss_index'] is None else st.session_state['faiss_index'].ntotal,
                            'retrieved_raw_count': len(results),
                            'retrieved_reranked_count': len(reranked)
                        })
                
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
